---
title: "HW1 - Esther Xuanpei Ouyang"
author: "Esther Xuanpei Ouyang"
date: "1/26/2018"
output:
  pdf_document: default
  html_document: default
subtitle: STAT 154 Lab 101
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
```{r}
X = matrix(c(2,3,-1,4), nrow = 2, ncol = 2)
X

Y = matrix(c(2,0,1,1,-2,3), nrow = 2, ncol = 3)
Y

Z = matrix(c(1,1,-1,1,0,2), nrow = 3, ncol = 2)
Z

W = matrix(c(1,0,8,3), nrow = 2, ncol = 2)
W  

I = matrix(c(1,0,0,1), nrow = 2, ncol = 2)
I
```
## Problem 2


### (a)
```{r}
# X + Y
# Cannot be performed because the dimension of X and Y are not the same.
```
### (b)
```{r}
X + W
```

### (c)
```{r}
X - I
```

### (d)
```{r}
X%*%Y
```

### (e)
```{r}
X%*%I
```

### (f)
$X + (Y + Z)$ cannot be performed because the dimension of the three matrices $X, Y, Z$ are not the same.

### (g)
$Y(I+W)$ cannot be performed because the dimension of $(I+W)$ is 2 by 2 and the dimension of $Y$ is 2 by 3, and the col number of $Y$ does not match with the row number of $(I+W)$.

## Problem 3

#### (a) Every orthogonal matrix is nonsingular.

True. 

By the definition of orthogonal matrix, for orthogonal matrix $Q$, the product of $Q^T$ and $Q$, $Q^TQ = I$. Also, by the definition of nonsingular matrix $A$, a matrix is nonsingular if there exists a matrix $B$ such that $AB=I$. Therefore, every orthogonal matrix $Q$ is a nonsingular matrix since its transpose $Q^T$ always exists and $Q^TQ=I$.

#### (b) Every nonsingular matrix is orthogonal.
False.

By the definition of orthogonal matrix, for for orthogonal matrix $Q$, $Q^TQ=QQ^T= I$.
A counter-example is that the nonsingular matrix $\begin{bmatrix} \end{bmatrix}$
Therefore, not all nonsingular matrix is orthogonal.


#### (c) Every matrix of full rank is square.
False

By the definition of rank, a matrix m by n matrix A is a full rank rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns, i.e., $rank(A)=min(m,n)$. Some m by n rectangular matrix can have $rank=min(m, n)$ as long as it has all linearly indepedent vectors as its rows if $m<n$ or columns if $m>n$.
A counter-example is that the matrix $X=\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix}$ has $rank=2$, which satisify that $rank(X)=min(2,3)=2$.
Therefore, not all matrix of full rank is square.


#### (d) Every square matrix is of full rank.
False

Not all square matrix is of full rank. A counter-example is that the matrix $A = \begin{bmatrix} 1 & 2 \\ 2 & 4\end{bmatrix}$ is a 2 by 2 square matrix but not of full rank since the row vectors of $A$ are linearly dependent. 


#### (e) Every nonsingular matrix is of full rank.
True

By the invertible matrix theorem, a matrix $A$ is a nonsinguar matrix is equivalent to $A$ is a full rank matrix. Therefore, every nonsingular matrix is of full rank.





## Problem 4
Want to Proof: $(XYZ)^T = Z^TY^TX^T$

We would like to prove that $(AB)^T = B^TA^T$, 
$$((AB)^T)_{ij} = (AB)_{ji} = \sum_k A_{jk}B_{ki} = \sum_k(A^T)_{kj}(B^T)_{ik} = \sum_k(B^T)_ik (A^T)_{kj} = (B^TA^T)_{ij} \text{ (1)}$$
Therefore, $(AB)^T = B^TA^T$

Let $A = XY, B = Z$, 
$$(XYZ)^T = (AB)^T \\ = B^TA^T \quad \text{by (1)} \\ =(Z)^T(XY)^T \quad \text{(plug in A,B)} \\ =Z^TY^TX^T \quad \text{by (1)}$$

## Problem 5

Consider the eigenvalue decomposition of a n by n symmetric matrix $A$, Prove that two eigenvectors $v_i$ and $v_j$ associated with two distinct eigenvalues $\lambda_i$ and $\lambda_j$ of $A$ are mutually orthogonal; that is , $v_i^Tv_j = 0$.

By the definition of eigenvalues and eigenvectors, $A\vec{q}=\lambda \vec{q}$.
Since A is a symmetric matrix, we can do eigendecomposition on matrix $A$. Here, I denote the orthonormal basis of eigenvectors as $q_1, \dots, q_n$ and their corresponding eigenvalues $\lambda_1, \dots, \lambda_n$. By eigendecomposition, $$A = U\Lambda U^T$$ where $U$ is a orthogonal matrix with $q_1, \dots, q_n$ as its columns and $\Lambda$ is a diagonal matrix $diag(\lambda_1, \dots, \lambda_n)$. 
Since by definition $Aq_i = \lambda_i q_i$ for every i, for every column `q_i` of $U$, i.e., eigenvectors of $U$ which corresponding to a distinct eigenvalues $\lambda_i$ in $\Lambda$. Also, $U$ is a orthonormal matrix, its columns are all orthogonal to each other, i.e., $q_i^Tq_j = 0 \text{ for } i \neq j$.

## Problem 6

### Problem 6.1
```{r}
inner_product = function (v, u) {
  return (v%*%u)
}
v = c(1, 3, 5)
u = c(1, 2, 3)
inner_product(v, u)
```
### Problem 6.2
```{r}
projection = function (v, u) {
  numer = inner_product(u, v)
  denom = inner_product(u, u)
  return ((numer/denom)*u)
}
v = c(1,3,5)
u = c(1,2,3)
projection(v, u)
```

## Problem 7
```{r}
vnorm = function(x) {
  return(sqrt(t(x)%*%x))
}

x = c(1, 2, 3)
y = c(3, 0, 2)
z = c(3, 1, 1)
# Start by setting u1 = x, and report the set of vectors uk and the orthonormalized vectors uk, for k = 1,2,3.
u1 = x
u1
u2 = y - projection(y, u1)
u2
u3 = z - projection(z, u1) - projection(z, u2)
u3

e1 = u1 / vnorm(u1)
e1
e2 = u2 / vnorm(u2)
e2
e3 = u3 / vnorm(u3)
e3
```
## Problem 8
```{r}
# function for computing L_p norm of a vector
#  
# x - the input vector, p - the value for p
lp_norm = function(x, p = 1) {
  if (p == "max") {
    return(max(abs(x)))
  } else {
    tot_sum = 0
    for ( val in x ){
      tot_sum = tot_sum + abs(val)^p
    }
    return(tot_sum^(1/p))
  }
}

y = matrix(-5:4,10)
y
lp_norm(y) # default p = 1 
lp_norm(y, p = 2)
lp_norm(y, p = "max") # L-max norm
```
## Problem 9
### (a)
```{r}
zero = rep(0, 10)
lp_norm(zero, 1)
```
### (b)

```{r}
ones = rep(1, 5)
lp_norm(ones, 2)
```

### (c)
```{r}
u = rep(0.4472136, 5)
lp_norm(u, 2)
```

### (d)
```{r}
u = 1:500
lp_norm(u, 100)
```
### (e)
```{r}
u = 1:500
lp_norm(u, "max")
```

## Problem 10
Consider the eigendecomposition of a square matrix $A$.

Since A is a symmetric matrix, we can do eigendecomposition on matrix $A$. Here, I denote the orthonormal basis of eigenvectors as $q_1, \dots, q_n$ and their corresponding eigenvalues $\lambda_1, \dots, \lambda_n$. By eigendecomposition, $$A = U\Lambda U^T$$ where $U$ is a orthogonal matrix with $q_1, \dots, q_n$ as its columns and $\Lambda$ is a diagonal matrix $diag(\lambda_1, \dots, \lambda_n)$. 
Since by definition $Aq_i = \lambda_i q_i$ for every i, for every column `q_i` of $U$, i.e., eigenvectors of $U$ which corresponding to a eigenvalues $\lambda_i$ in $\Lambda$.

$U = \begin{bmatrix} \vert & & \vert \\ q_1  & \dots & q_n   \\ \vert & & \vert \end{bmatrix}, \text{where } q_i \text{ is the i column vector of U}, \Lambda = \begin{bmatrix} \lambda_{1} & & 0\\ & \ddots & \\0 & & \lambda_{n}\end{bmatrix}$.

### a. Prove that the matrix $bA$, where $b$ is an arbitrary scalar, has $b\lambda$ as an eigenvalue, with $v$ as the associated eigenvector.

By the definition of eigenvalues and eigenvectors, $$A\vec{q_i}=\lambda_i \vec{q_i}$$
Then, multiple both sides by scalar $b$, 
$$bA\vec{q_i}=k\lambda_i\vec{q_i} \\ \Rightarrow (bA)\vec{q_i}=(k\lambda_i)\vec{q_i}$$

Again, by definition of eigenvalues and eigenvectors, for matrix $bA$, $\vec{q}$ and $k\lambda$ are the corresponding eigenvectors and eigenvalues.

We can also use eigendecomposition of A to prove.
$$A=U\Lambda U^T$$
$$bA=b(U\Lambda U^T)=(U)(b\Lambda)(U^T)$$

where $b\Lambda = \begin{bmatrix} b\lambda_{1} & & 0\\ & \ddots & \\0 & & b\lambda_{n}\end{bmatrix}$.

Therefore, by eigendecomposition, the matrix $bA$ has $\vec{q}$ as an eigenvector and $b\lambda$ as an eigenvalue.

### b. Prove that the matrix $A+cI$, where $c$ is an arbitrary scalar, has $(\lambda+c)$ as an eigenvalue, with $v$ as the associated eigenvector.

By the definition of eigenvalues and eigenvectors, $$A\vec{q_i}=\lambda_i \vec{q_i}$$ 
Here, let $A' = A+cI$ and plug in, $$(A+cI)\vec{q_i}=A\vec{q_i}+cI\vec{q_i} \\ =\lambda_i \vec{q_i} + c\vec{q_i}\text{ (since } A\vec{q_i} = \lambda_i \vec{q_i} \text{ )} \\ = (\lambda_i+c) \vec{q_i} \text{ (since } c, \lambda_i \text{ are both scalar value)}$$

Therefore, 
$$A'\vec{q_i} = (\lambda_i+c)\vec{q_i}$$
Again, by the definition of eigenvalues and eigenvectors, the matrix $A'=A+cI$ has $vec{q_i}$ as an eigenvector and $(\lambda_i+c)$ as an eigenvalues. 


## Problem 11

### (a) 
Select the first five columns of state.x77 and convert them as a matrix; this will be the data matrix X. Let $n$ be the number of rows of $X$, and $p$ the number of columns of $X$.

```{r}
head(state.x77, 5)
X = matrix(head(state.x77, 5), nrow = 5)
X
# number of rows of X
n = dim(X)[1]
n
# number of columns of X
p = dim(X)[2]
p
```
### (b) 
Create a diagonal matrix $D = \frac{1}{n}I$ where $I$ is the $n x n$ identity matrix. Display the output of $sum(diag(D))$.

```{r}
D = diag(1, n)/n
D
sum(diag(D))
```
### (c)
Compute the vector of column means $g = X^TD1$ where $1$ is a vector of 1â€™s of length $n$. Display (i.e. print) $g$.

```{r}
ones = rep(1, n)
ones
g = t(X)%*%D%*%ones
g
```
### (d)
Calculate the mean-centered matrix $X_c=X-1g^T$. Display the output of $colMeans(Xc)$.
```{r}
Xc = X - ones%*%t(g)
Xc
colMeans(Xc)
```
### (e)
Compute the (population) variance-covariance matrix $V=X^TDX-gg^T$. Display the output of V.
```{r}
V = t(X)%*%D%*%X - g%*%t(g)
V
```
### (f)
Display only the elements in the diagonal of $D_{1/S}$.
```{r}
diag(V)
D_1S = 1/diag(V)
D_1S
```
### (g)
Display the output of $colMeans(Z)$ and $apply(Z, 2, var)$
```{r}
Z = Xc%*%D_1S
Z
colMeans(Z)
apply(Z, 2, var)
```

### (h)
Compute the (population) correlation matrix $R = D_{1/S} VD_{1/S}$. Display the matrix $R$.
```{r}
R = D_1S%*%V%*%D_1S
R
```

### (i)
Confirm that R can also be obtained as $R = Z^TDZ$.
```{r}
R = t(Z)%*%D%*%Z
R
```

## Comments and Reflections
1. Math part, such as eigen-decomposition and singular value decomposition.
2. R programming.
3. Yes, I use Google and textbook resource.
4. Around 4 hours.
5. Problem 11
